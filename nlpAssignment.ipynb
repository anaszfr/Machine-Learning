{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1BpMyKjocN2",
        "outputId": "741ba793-dbcf-4d0b-e5e5-c5e795212269"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized Documents:\n",
            "[['natural', 'language', 'processing', 'fascinating', 'field', 'ai'], ['machine', 'learning', 'nlp', 'closely', 'related'], ['tfidf', 'ngrams', 'essential', 'techniques', 'nlp']]\n",
            "\n",
            "Bag of Words Representation:\n",
            "   ai  closely  essential  fascinating  field  language  learning  machine  \\\n",
            "0   1        0          0            1      1         1         0        0   \n",
            "1   0        1          0            0      0         0         1        1   \n",
            "2   0        0          1            0      0         0         0        0   \n",
            "\n",
            "   natural  ngrams  nlp  processing  related  techniques  tfidf  \n",
            "0        1       0    0           1        0           0      0  \n",
            "1        0       0    1           0        1           0      0  \n",
            "2        0       1    1           0        0           1      1  \n",
            "\n",
            "TF-IDF Representation:\n",
            "         ai   closely  essential  fascinating     field  language  learning  \\\n",
            "0  0.408248  0.000000   0.000000     0.408248  0.408248  0.408248  0.000000   \n",
            "1  0.000000  0.467351   0.000000     0.000000  0.000000  0.000000  0.467351   \n",
            "2  0.000000  0.000000   0.467351     0.000000  0.000000  0.000000  0.000000   \n",
            "\n",
            "    machine   natural    ngrams       nlp  processing   related  techniques  \\\n",
            "0  0.000000  0.408248  0.000000  0.000000    0.408248  0.000000    0.000000   \n",
            "1  0.467351  0.000000  0.000000  0.355432    0.000000  0.467351    0.000000   \n",
            "2  0.000000  0.000000  0.467351  0.355432    0.000000  0.000000    0.467351   \n",
            "\n",
            "      tfidf  \n",
            "0  0.000000  \n",
            "1  0.000000  \n",
            "2  0.467351  \n",
            "\n",
            "Bi-Grams:\n",
            "[[('natural', 'language'), ('language', 'processing'), ('processing', 'fascinating'), ('fascinating', 'field'), ('field', 'ai')], [('machine', 'learning'), ('learning', 'nlp'), ('nlp', 'closely'), ('closely', 'related')], [('tfidf', 'ngrams'), ('ngrams', 'essential'), ('essential', 'techniques'), ('techniques', 'nlp')]]\n",
            "\n",
            "Tri-Grams:\n",
            "[[('natural', 'language', 'processing'), ('language', 'processing', 'fascinating'), ('processing', 'fascinating', 'field'), ('fascinating', 'field', 'ai')], [('machine', 'learning', 'nlp'), ('learning', 'nlp', 'closely'), ('nlp', 'closely', 'related')], [('tfidf', 'ngrams', 'essential'), ('ngrams', 'essential', 'techniques'), ('essential', 'techniques', 'nlp')]]\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from nltk.util import ngrams\n",
        "import pandas as pd\n",
        "\n",
        "# Predefined stop words list\n",
        "STOP_WORDS = set([\n",
        "    'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves',\n",
        "    'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their',\n",
        "    'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
        "    'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an',\n",
        "    'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about',\n",
        "    'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up',\n",
        "    'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when',\n",
        "    'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no',\n",
        "    'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don',\n",
        "    'should', 'now'\n",
        "])\n",
        "\n",
        "# Sample data\n",
        "documents = [\n",
        "    \"Natural Language Processing is a fascinating field of AI.\",\n",
        "    \"Machine Learning and NLP are closely related\",\n",
        "    \"TF-IDF and N-Grams are essential techniques in NLP\"\n",
        "]\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_text(doc):\n",
        "    doc = doc.lower()\n",
        "    doc = doc.translate(str.maketrans('', '', string.punctuation))\n",
        "    tokens = doc.split()\n",
        "    tokens = [word for word in tokens if word not in STOP_WORDS]\n",
        "    return tokens\n",
        "\n",
        "# Preprocessed documents\n",
        "preprocessed_docs = [' '.join(preprocess_text(doc)) for doc in documents]\n",
        "\n",
        "# 1. Tokenization\n",
        "def tokenize_text(documents):\n",
        "    return [doc.split() for doc in documents]\n",
        "\n",
        "print(\"Tokenized Documents:\")\n",
        "print(tokenize_text(preprocessed_docs))\n",
        "\n",
        "# 2. Bag of Words (BoW)\n",
        "vectorizer_bow = CountVectorizer()\n",
        "X_bow = vectorizer_bow.fit_transform(preprocessed_docs)\n",
        "bow_df = pd.DataFrame(X_bow.toarray(), columns=vectorizer_bow.get_feature_names_out())\n",
        "\n",
        "print(\"\\nBag of Words Representation:\")\n",
        "print(bow_df)\n",
        "\n",
        "# 3. TF-IDF\n",
        "vectorizer_tfidf = TfidfVectorizer()\n",
        "X_tfidf = vectorizer_tfidf.fit_transform(preprocessed_docs)\n",
        "tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=vectorizer_tfidf.get_feature_names_out())\n",
        "\n",
        "print(\"\\nTF-IDF Representation:\")\n",
        "print(tfidf_df)\n",
        "\n",
        "# 4. N-Grams\n",
        "def generate_ngrams(documents, n):\n",
        "    ngram_list = []\n",
        "    for doc in documents:\n",
        "        tokens = doc.split()\n",
        "        ngram_list.append(list(ngrams(tokens, n)))\n",
        "    return ngram_list\n",
        "\n",
        "bi_grams = generate_ngrams(preprocessed_docs, 2)\n",
        "tri_grams = generate_ngrams(preprocessed_docs, 3)\n",
        "\n",
        "print(\"\\nBi-Grams:\")\n",
        "print(bi_grams)\n",
        "\n",
        "print(\"\\nTri-Grams:\")\n",
        "print(tri_grams)\n",
        "\n"
      ]
    }
  ]
}